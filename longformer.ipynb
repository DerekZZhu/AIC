{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/data_3class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core(s):\n",
    "    try:\n",
    "        start = s.index(\"Containment Procedures:\")\n",
    "        return s[start:start+512]\n",
    "    except:\n",
    "        return s[len(s)//2-106:len(s)//2+106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>item</th>\n",
       "      <th>class</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SCP-6501</td>\n",
       "      <td>safe</td>\n",
       "      <td>Containment Procedures: The cemetery in which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SCP-6502</td>\n",
       "      <td>safe</td>\n",
       "      <td>Containment Procedures: To maximize available ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SCP-6503</td>\n",
       "      <td>keter</td>\n",
       "      <td>ch as an epic poem or a warrior's creed have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SCP-6504</td>\n",
       "      <td>euclid</td>\n",
       "      <td>Containment Procedures: Global psychiatric res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SCP-6506</td>\n",
       "      <td>safe</td>\n",
       "      <td>Containment Procedures: SCP-6506 is currently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>8214</td>\n",
       "      <td>SCP-7995</td>\n",
       "      <td>euclid</td>\n",
       "      <td>Containment Procedures: All information regard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>8215</td>\n",
       "      <td>SCP-7996</td>\n",
       "      <td>keter</td>\n",
       "      <td>Containment Procedures: Due to the unpredictab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>8216</td>\n",
       "      <td>SCP-7997</td>\n",
       "      <td>keter</td>\n",
       "      <td>Containment Procedures: The Department of Proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>8217</td>\n",
       "      <td>SCP-7998</td>\n",
       "      <td>keter</td>\n",
       "      <td>Containment Procedures: Due to a lack of knowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>8218</td>\n",
       "      <td>SCP-7999</td>\n",
       "      <td>euclid</td>\n",
       "      <td>Containment Procedures: As SCP-7999 has alread...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7679 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      item   class  \\\n",
       "0              0  SCP-6501    safe   \n",
       "1              1  SCP-6502    safe   \n",
       "2              2  SCP-6503   keter   \n",
       "3              3  SCP-6504  euclid   \n",
       "4              5  SCP-6506    safe   \n",
       "...          ...       ...     ...   \n",
       "7674        8214  SCP-7995  euclid   \n",
       "7675        8215  SCP-7996   keter   \n",
       "7676        8216  SCP-7997   keter   \n",
       "7677        8217  SCP-7998   keter   \n",
       "7678        8218  SCP-7999  euclid   \n",
       "\n",
       "                                                 report  \n",
       "0     Containment Procedures: The cemetery in which ...  \n",
       "1     Containment Procedures: To maximize available ...  \n",
       "2     ch as an epic poem or a warrior's creed have b...  \n",
       "3     Containment Procedures: Global psychiatric res...  \n",
       "4     Containment Procedures: SCP-6506 is currently ...  \n",
       "...                                                 ...  \n",
       "7674  Containment Procedures: All information regard...  \n",
       "7675  Containment Procedures: Due to the unpredictab...  \n",
       "7676  Containment Procedures: The Department of Proc...  \n",
       "7677  Containment Procedures: Due to a lack of knowl...  \n",
       "7678  Containment Procedures: As SCP-7999 has alread...  \n",
       "\n",
       "[7679 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "mid = df\n",
    "mid[\"report\"] = mid[\"report\"].apply(get_core)\n",
    "mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid.to_csv(\"data3_core.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df['report'] = df['report'].dropna()\n",
    "df['class'] = df['class'].astype('category').cat.codes\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['report'].tolist(),\n",
    "    df['class'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223995131e30464fae0b564c594959d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861cf97f7dca43babd22dfd26d367ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8348290556c44187bbc7eea8b3993bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea1afb30dd4ff2aa238cc03da317d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=1024)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.labels = labels\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item \n",
    "\n",
    "train_dataset = SCPDataset(train_encodings, train_labels)\n",
    "val_dataset = SCPDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  1%|          | 1/192 [05:06<16:16:28, 306.75s/it]"
     ]
    }
   ],
   "source": [
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=df['class'].nunique())\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=df['class'].nunique())\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# optimizer = SGD(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "epochs = 3\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss/ len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {e + 1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/SCP-Classifications-Longformer-epoch3\", from_pt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
